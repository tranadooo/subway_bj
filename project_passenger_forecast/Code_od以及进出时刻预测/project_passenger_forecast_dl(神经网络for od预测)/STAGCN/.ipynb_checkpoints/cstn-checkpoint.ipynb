{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\" \n",
    "Usage:\n",
    "    python trainval.py -h\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "from datetime import datetime\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,TensorBoard,LearningRateScheduler,Callback\n",
    "from utils.dataset import load_data\n",
    "import models\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import plot_model as plot\n",
    "from keras.utils import multi_gpu_model\n",
    "\n",
    "import utils\n",
    "import utils.metrics as Metrics\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# uncomment followng to set fix random seed\n",
    "np.random.seed(1337)\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, help='model to train and eval')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learing rate')\n",
    "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
    "parser.add_argument('--seq_len', type=int, default=5, help='length of import sequence')\n",
    "parser.add_argument('--pre_train', type=bool, default=False, help='whether to load weights file or not')\n",
    "parser.add_argument('--weights', type=str, help='weights file to load')\n",
    "parser.add_argument('--gpus', type=str, help='gpus to use, auto parallelize')\n",
    "\n",
    "\n",
    "def get_tensorboard(path):\n",
    "    tensorboard = TensorBoard(log_dir=path)\n",
    "    return tensorboard\n",
    "\n",
    "def save_file(file, path):\n",
    "    rtcode = os.system(\" \".join([\"cp\", file.replace(\".pyc\", \".py\"), path]))\n",
    "    assert rtcode == 0\n",
    "\n",
    "def get_decay(base_lr):\n",
    "    def step_decay_lr(epoch):\n",
    "        if epoch < 200:\n",
    "            return base_lr\n",
    "        else:\n",
    "            return base_lr * 0.1 \n",
    "\n",
    "    return step_decay_lr\n",
    "\n",
    "\n",
    "def show_score(odmax, score, stage):\n",
    "    print(stage + ' score: %.6f rmse (real): %.6f mape: %.6f' %\n",
    "          (score[0], score[1], score[2]))\n",
    "\n",
    "    print('origin rmse (real): %.6f mape: %.6f' %\n",
    "          (score[3], score[4]))\n",
    "\n",
    "class SGDLearningRateTracker(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        lr = float(K.get_value(optimizer.lr))\n",
    "        print('LR: {:.6f}'.format(lr))\n",
    "\n",
    "\n",
    "def rmse(a, b):\n",
    "    return Metrics.rmse(a, b) * 241.0 / 2.0\n",
    "def o_rmse(a, b):\n",
    "    return Metrics.o_rmse(a, b) * 241.0 / 2.0\n",
    "\n",
    "def train(model, lr, batch_size, seq_len, pre_train, weights, DEMODEL):\n",
    "    \n",
    "#     import pdb;pdb.set_trace()\n",
    "    odmax = 241\n",
    "    use_tensorboard = True\n",
    "    gpu_count = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\n",
    "    parallel = True if gpu_count != 1 else False\n",
    "\n",
    "    nb_epoch = 200       # number of epoch at training stage\n",
    "    nb_epoch_cont = 500  # number of epoch at continued training stage\n",
    "\n",
    "    T = 48  # number of time intervals in one day\n",
    "    m_patience = 20 # number of epoch to train \n",
    "    timestep = seq_len #5\n",
    "    map_height, map_width = 15, 5  # grid size\n",
    "\n",
    "    days_test = 60\n",
    "\n",
    "    pt = datetime.now().strftime('%m_%d_%H_%M_%S')\n",
    "    path_model = 'TRAIN/' + pt\n",
    "    if os.path.isdir(path_model) is False:\n",
    "        os.makedirs(path_model)\n",
    "    print(\"Exp: \" + path_model)\n",
    "\n",
    "    # load data\n",
    "    print(\"loading data...\")\n",
    "    '''\n",
    "        expect:\n",
    "        X = (sample, timestep, map_height * map_width, map_height, map_width)\n",
    "        Y = (sample, map_height * map_width, map_height, map_width)\n",
    "        weather = (sample, timestep, ?)\n",
    "        meta = (sample, timestep, ?)\n",
    "\n",
    "        The meta data is not used in this work, but we can explore its effect in future works. \n",
    "    '''\n",
    "    X, Y, weather, meta = load_data(odmax, timestep)\n",
    "    len_test = T * days_test\n",
    "\n",
    "\n",
    "\n",
    "    print(\"nb_epoch: \" + str(nb_epoch) + \" nb_epoch_cont: \" + str(nb_epoch_cont) + \" batch_size: \" + str(batch_size))\n",
    "    print(\"patience: \" + str(m_patience) + \" lr: \" + str(lr) + \" seq_len: \" + str(timestep))# + '-' + str(len_period) + '-' + str(len_trend))\n",
    "    print(\"odmax: \" + str(odmax))\n",
    "    print(\"{} sample totally. {} for train, {} for test\".format(X.shape[0], X.shape[0] - len_test, len_test))\n",
    "\n",
    "    X_train, X_test = X[:-len_test], X[-len_test:]\n",
    "    Y_train, Y_test = Y[:-len_test], Y[-len_test:]\n",
    "    weather_train, weather_test = weather[:-len_test], weather[-len_test:]\n",
    "    meta_train, meta_test = meta[:-len_test], meta[-len_test:]\n",
    "\n",
    "    X_train = [X_train, weather_train, meta_train]\n",
    "    X_test = [X_test, weather_test, meta_test]\n",
    "\n",
    "    \n",
    "\n",
    "    \"\"\"********************************************************************************************\"\"\"\n",
    "    \"\"\" Frist, we train our model with fixed learning rate                                         \"\"\"\n",
    "    \"\"\"********************************************************************************************\"\"\"\n",
    "\n",
    "    model_para = {\n",
    "        \"timestep\": timestep,\n",
    "        \"map_height\": map_height,\n",
    "        \"map_width\": map_width,\n",
    "        \"weather_dim\": weather.shape[2],\n",
    "        \"meta_dim\": meta.shape[2],\n",
    "    }\n",
    "    # Build the model to train in parallel with multi-GPUs or only on GPU\n",
    "    if parallel:\n",
    "        model = DEMODEL.build_model(**model_para)\n",
    "        plot(model, to_file=os.path.join(path_model,'networks.png'), show_shapes=True)\n",
    "        model.summary()\n",
    "        train_model = multi_gpu_model(model, gpu_count)\n",
    "\n",
    "    else:\n",
    "        model = DEMODEL.build_model(**model_para)\n",
    "        plot(model, to_file=os.path.join(path_model,'networks.png'), show_shapes=True)\n",
    "        model.summary()\n",
    "        train_model = model\n",
    "\n",
    "    # use the loss define in the model\n",
    "    loss = DEMODEL.get_loss()\n",
    "    optimizer = Adam(lr=lr)\n",
    "\n",
    "    metrics = [ rmse, Metrics.mape,  \\\n",
    "                o_rmse, Metrics.o_mape,\n",
    "                ]\n",
    "    train_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    # load weights to the pre_train model after model compiled\n",
    "    if pre_train == True:\n",
    "        model.load_weights(weights, by_name=True, skip_mismatch=True)\n",
    "\n",
    "    # define callbacks on training\n",
    "    callbacks = []\n",
    "\n",
    "    hyperparams_name = 'timestep{}.lr{}'.format(timestep, lr)\n",
    "    fname_param = os.path.join(path_model, hyperparams_name + '.best.h5')\n",
    "    lr_logger = SGDLearningRateTracker() # log out the learning rate after a epoch trained\n",
    "    callbacks.append(lr_logger)\n",
    "    callbacks.append(EarlyStopping(monitor='val_rmse', patience=m_patience, mode='min'))\n",
    "    callbacks.append(ModelCheckpoint(\n",
    "        fname_param, monitor='val_mape', verbose=0, save_best_only=True, mode='min'))\n",
    "\n",
    "    if use_tensorboard:\n",
    "        callbacks.append(get_tensorboard(path_model+\"/tensorboard-1/\"))\n",
    "\n",
    "    print('=' * 10)\n",
    "    print(\"training model...\")\n",
    "    history = train_model.fit(X_train, Y_train,\n",
    "                        nb_epoch=nb_epoch,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=1)\n",
    "\n",
    "    model.save_weights(os.path.join(\n",
    "        path_model, '{}.h5'.format(hyperparams_name)), overwrite=True)\n",
    "    train_model.load_weights(fname_param)\n",
    "    model.save_weights(fname_param, overwrite=True)\n",
    "    pickle.dump((history.history), open(os.path.join(\n",
    "        path_model, '{}.history.pkl'.format(hyperparams_name)), 'wb'))\n",
    "\n",
    "    print('evaluating using the model that has the best model on the valid set')\n",
    "\n",
    "    model.load_weights(fname_param)\n",
    "    \n",
    "    score = train_model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=0)\n",
    "    show_score(odmax, score, \"train\")\n",
    "    score = train_model.evaluate(\n",
    "        X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "    show_score(odmax, score, \"Test\")\n",
    "\n",
    "    print('=' * 10)\n",
    "\n",
    "\n",
    "    \"\"\"********************************************************************************************\"\"\"\n",
    "    \"\"\" Second, we train our model with step_decay learning rate                                   \"\"\"\n",
    "    \"\"\"********************************************************************************************\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # clear session to rebuild the model, in order to switch optimizor\n",
    "    K.clear_session()\n",
    "    DEMODEL.clear_graph()\n",
    "\n",
    "    # rebuild the model\n",
    "    if parallel:\n",
    "        model = DEMODEL.build_model(**model_para)\n",
    "        train_model = multi_gpu_model(model, gpu_count)\n",
    "    else:\n",
    "        model = DEMODEL.build_model(**model_para)\n",
    "        train_model = model\n",
    "\n",
    "    loss = DEMODEL.get_loss()\n",
    "    optimizer = Adam(lr=lr)\n",
    "    metrics = [ rmse, Metrics.mape, \\\n",
    "                o_rmse, Metrics.o_mape,\n",
    "                ]\n",
    "    train_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    model.load_weights(fname_param)\n",
    "\n",
    "    fname_param_step =  os.path.join(\n",
    "        path_model, \\\n",
    "        hyperparams_name + '.cont.best.h5.{epoch:03d}-{val_mape:.4f}-{val_rmse:.4f}-{val_o_mape:.4f}-{val_o_rmse:.4f}')\n",
    "    callbacks_cont = []\n",
    "    #lr_logger = SGDLearningRateTracker()\n",
    "\n",
    "\n",
    "    # callbacks_cont.append(lr_logger)\n",
    "    callbacks_cont.append(LearningRateScheduler(get_decay(lr)))\n",
    "    callbacks_cont.append(ModelCheckpoint(\n",
    "        fname_param_step, monitor='val_mape', verbose=0, save_best_only=False, period=1, save_weights_only=True, mode='min'))\n",
    "    if use_tensorboard:\n",
    "        callbacks_cont.append(get_tensorboard(path_model+\"/tensorboard-2/\"))\n",
    "\n",
    "    history = train_model.fit(X_train, Y_train,\n",
    "                        nb_epoch=nb_epoch_cont, \n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=callbacks_cont, \n",
    "                        validation_data=(X_test, Y_test),\n",
    "                        verbose=1)\n",
    "\n",
    "    pickle.dump((history.history), open(os.path.join(\n",
    "        path_model, '{}.cont.history.pkl'.format(hyperparams_name)), 'wb'))\n",
    "    model.save_weights(os.path.join(\n",
    "        path_model, '{}_cont.h5'.format(hyperparams_name)), overwrite=True)\n",
    "    model.load_weights(fname_param)\n",
    "    model.save_weights(fname_param, overwrite=True) # save the origin model weights instead of the paralleled one\n",
    "\n",
    "    print('=' * 10)\n",
    "    print('evaluating using the final model')\n",
    "    score = train_model.evaluate(X_train, Y_train, batch_size=32, verbose=0)\n",
    "    show_score(odmax, score, \"train\")\n",
    "    score = train_model.evaluate(\n",
    "        X_test, Y_test, batch_size=32, verbose=0)\n",
    "    show_score(odmax, score, \"test\")\n",
    "    \"\"\"\n",
    "    \n",
    "model = \"CSTN\"\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "gpus = '0'\n",
    "seq_len=5\n",
    "pre_train=False\n",
    "weights=None\n",
    "\n",
    "os.environ[\"HDF5_DISABLE_VERSION_CHECK\"]='1'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "\n",
    "import_net=\"import models.%s as DEMODEL\"%(model)\n",
    "exec(import_net)\n",
    "\n",
    "# train(model, lr, batch_size, seq_len, pre_train, weights, DEMODEL)\n",
    "\n",
    "odmax = 241\n",
    "use_tensorboard = True\n",
    "gpu_count = len(os.environ[\"CUDA_VISIBLE_DEVICES\"].split(','))\n",
    "parallel = True if gpu_count != 1 else False\n",
    "\n",
    "nb_epoch = 200       # number of epoch at training stage\n",
    "nb_epoch_cont = 500  # number of epoch at continued training stage\n",
    "\n",
    "T = 48  # number of time intervals in one day\n",
    "m_patience = 20 # number of epoch to train \n",
    "timestep = seq_len #5\n",
    "map_height, map_width = 15, 5  # grid size\n",
    "\n",
    "days_test = 60\n",
    "\n",
    "pt = datetime.now().strftime('%m_%d_%H_%M_%S')\n",
    "path_model = 'TRAIN/' + pt\n",
    "if os.path.isdir(path_model) is False:\n",
    "    os.makedirs(path_model)\n",
    "print(\"Exp: \" + path_model)\n",
    "\n",
    "# load data\n",
    "print(\"loading data...\")\n",
    "'''\n",
    "    expect:\n",
    "    X = (sample, timestep, map_height * map_width, map_height, map_width)\n",
    "    Y = (sample, map_height * map_width, map_height, map_width)\n",
    "    weather = (sample, timestep, ?)\n",
    "    meta = (sample, timestep, ?)\n",
    "\n",
    "    The meta data is not used in this work, but we can explore its effect in future works. \n",
    "'''\n",
    "X, Y, weather, meta = load_data(odmax, timestep)\n",
    "len_test = T * days_test\n",
    "\n",
    "\n",
    "\n",
    "print(\"nb_epoch: \" + str(nb_epoch) + \" nb_epoch_cont: \" + str(nb_epoch_cont) + \" batch_size: \" + str(batch_size))\n",
    "print(\"patience: \" + str(m_patience) + \" lr: \" + str(lr) + \" seq_len: \" + str(timestep))# + '-' + str(len_period) + '-' + str(len_trend))\n",
    "print(\"odmax: \" + str(odmax))\n",
    "print(\"{} sample totally. {} for train, {} for test\".format(X.shape[0], X.shape[0] - len_test, len_test))\n",
    "\n",
    "X_train, X_test = X[:-len_test], X[-len_test:]\n",
    "Y_train, Y_test = Y[:-len_test], Y[-len_test:]\n",
    "weather_train, weather_test = weather[:-len_test], weather[-len_test:]\n",
    "meta_train, meta_test = meta[:-len_test], meta[-len_test:]\n",
    "\n",
    "X_train = [X_train, weather_train, meta_train]\n",
    "X_test = [X_test, weather_test, meta_test]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"********************************************************************************************\"\"\"\n",
    "\"\"\" Frist, we train our model with fixed learning rate                                         \"\"\"\n",
    "\"\"\"********************************************************************************************\"\"\"\n",
    "\n",
    "model_para = {\n",
    "    \"timestep\": timestep,\n",
    "    \"map_height\": map_height,\n",
    "    \"map_width\": map_width,\n",
    "    \"weather_dim\": weather.shape[2],\n",
    "    \"meta_dim\": meta.shape[2],\n",
    "}\n",
    "# Build the model to train in parallel with multi-GPUs or only on GPU\n",
    "if parallel:\n",
    "    model = DEMODEL.build_model(**model_para)\n",
    "    plot(model, to_file=os.path.join(path_model,'networks.png'), show_shapes=True)\n",
    "    model.summary()\n",
    "    train_model = multi_gpu_model(model, gpu_count)\n",
    "\n",
    "else:\n",
    "    model = DEMODEL.build_model(**model_para)\n",
    "    plot(model, to_file=os.path.join(path_model,'networks.png'), show_shapes=True)\n",
    "    model.summary()\n",
    "    train_model = model\n",
    "\n",
    "# use the loss define in the model\n",
    "loss = DEMODEL.get_loss()\n",
    "optimizer = Adam(lr=lr)\n",
    "\n",
    "metrics = [ rmse, Metrics.mape,  \\\n",
    "            o_rmse, Metrics.o_mape,\n",
    "            ]\n",
    "train_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# load weights to the pre_train model after model compiled\n",
    "if pre_train == True:\n",
    "    model.load_weights(weights, by_name=True, skip_mismatch=True)\n",
    "\n",
    "# define callbacks on training\n",
    "callbacks = []\n",
    "\n",
    "hyperparams_name = 'timestep{}.lr{}'.format(timestep, lr)\n",
    "fname_param = os.path.join(path_model, hyperparams_name + '.best.h5')\n",
    "lr_logger = SGDLearningRateTracker() # log out the learning rate after a epoch trained\n",
    "callbacks.append(lr_logger)\n",
    "callbacks.append(EarlyStopping(monitor='val_rmse', patience=m_patience, mode='min'))\n",
    "callbacks.append(ModelCheckpoint(\n",
    "    fname_param, monitor='val_mape', verbose=0, save_best_only=True, mode='min'))\n",
    "\n",
    "if use_tensorboard:\n",
    "    callbacks.append(get_tensorboard(path_model+\"/tensorboard-1/\"))\n",
    "\n",
    "print('=' * 10)\n",
    "print(\"training model...\")\n",
    "history = train_model.fit(X_train, Y_train,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    callbacks=callbacks,\n",
    "                    verbose=1)\n",
    "\n",
    "model.save_weights(os.path.join(\n",
    "    path_model, '{}.h5'.format(hyperparams_name)), overwrite=True)\n",
    "train_model.load_weights(fname_param)\n",
    "model.save_weights(fname_param, overwrite=True)\n",
    "pickle.dump((history.history), open(os.path.join(\n",
    "    path_model, '{}.history.pkl'.format(hyperparams_name)), 'wb'))\n",
    "\n",
    "print('evaluating using the model that has the best model on the valid set')\n",
    "\n",
    "model.load_weights(fname_param)\n",
    "\n",
    "score = train_model.evaluate(X_train, Y_train, batch_size=batch_size, verbose=0)\n",
    "show_score(odmax, score, \"train\")\n",
    "score = train_model.evaluate(\n",
    "    X_test, Y_test, batch_size=batch_size, verbose=0)\n",
    "show_score(odmax, score, \"Test\")\n",
    "\n",
    "print('=' * 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_var(varname, value):\n",
    "    def decorate(func):\n",
    "        setattr(func, varname, value)\n",
    "        return func\n",
    "    return decorate\n",
    "\n",
    "@static_var(\"gcc_layers\", {1})\n",
    "def test():\n",
    "    def test1(layer):\n",
    "        if layer not in test.gcc_layers:\n",
    "            test.gcc_layers=test.gcc_layers|{layer}\n",
    "            print(test.gcc_layers)\n",
    "    def test2(b):\n",
    "        return test1(b+1)\n",
    "    return test2\n",
    "\n",
    "        \n",
    "\n",
    "a=test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 17 03:38:22 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.31       Driver Version: 440.31       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P40           Off  | 00000000:06:00.0 Off |                  Off |\n",
      "| N/A   44C    P0    50W / 250W |  23191MiB / 24451MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P40           Off  | 00000000:2F:00.0 Off |                    0 |\n",
      "| N/A   25C    P0    48W / 250W |      0MiB / 22919MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla P40           Off  | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    48W / 250W |      0MiB / 22919MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla P40           Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   27C    P0    47W / 250W |      0MiB / 22919MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CST-GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 邻接矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConstruct():\n",
    "    def __init__(self,\n",
    "                 num_nodes,\n",
    "                 edge,\n",
    "                 max_hop=40,\n",
    "                 strategy='distance'):\n",
    "        self.num_nodes=num_nodes;self.edge=edge\n",
    "        self.max_hop=max_hop;self.strategy=strategy\n",
    "        self.hop_distance = self.get_hop_distance(num_nodes, edge, max_hop=max_hop)\n",
    "        self.A=self.get_adjacency(self.hop_distance, strategy=self.strategy)\n",
    "        self.A=self.A[1:,...] #考虑到地铁自环的情况极少，所以去掉0阶\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_hop_distance(self, num_node, edge, max_hop=1):\n",
    "        # 通过edge数组计算有效邻接矩阵，max_hop=n表征了一种两点n步可达的能力\n",
    "        A = np.zeros((num_node, num_node),dtype=int)\n",
    "        for i, j in edge:\n",
    "            A[j, i] = 1\n",
    "            A[i, j] = 1\n",
    "\n",
    "        # compute hop steps\n",
    "        hop_dis = np.zeros((num_node, num_node)) + np.inf\n",
    "        #np.linalg.matrix_power(A, d)刻画两点d步可达的邻接矩阵\n",
    "        transfer_mat = [np.linalg.matrix_power(A, d) for d in range(max_hop + 1)]\n",
    "        arrive_mat = (np.stack(transfer_mat) > 0)\n",
    "        for d in range(max_hop, -1, -1):\n",
    "            hop_dis[arrive_mat[d]] = d\n",
    "        return hop_dis\n",
    "\n",
    "    def normalize_digraph(self, A):\n",
    "        Dl = np.sum(A, 0)\n",
    "        num_node = A.shape[0]\n",
    "        Dn = np.zeros((num_node, num_node))\n",
    "        for i in range(num_node):\n",
    "            if Dl[i] > 0:\n",
    "                Dn[i, i] = Dl[i] ** (-1)\n",
    "\n",
    "        DA = np.dot(Dn, A)\n",
    "        return DA\n",
    "\n",
    "\n",
    "\n",
    "    def get_adjacency(self, hop_dis, strategy, max_hop=1):\n",
    "        # 邻接矩阵的拆分策略\n",
    "        valid_hop = range(0, max_hop + 1)\n",
    "        adjacency = np.zeros((num_nodes, num_nodes))\n",
    "        for hop in valid_hop:\n",
    "            adjacency[hop_dis == hop] = 1\n",
    "\n",
    "\n",
    "        normalize_adjacency = self.normalize_digraph(adjacency)\n",
    "\n",
    "        # 1.公用一个邻接矩阵\n",
    "        if strategy == 'uniform':\n",
    "            A = np.zeros((1, num_nodes, num_nodes))\n",
    "            A[0] = normalize_adjacency\n",
    "\n",
    "        # 2.按距离拆分邻接矩阵，A[i][p,q]的值代表p站到q站的距离是否为i步，是为1，不是为0\n",
    "        elif strategy == 'distance':\n",
    "            A = np.zeros((len(valid_hop), num_nodes, num_nodes))\n",
    "            for i, hop in enumerate(valid_hop):\n",
    "                A[i][hop_dis == hop] = normalize_adjacency[hop_dis == hop]\n",
    "\n",
    "        return A\n",
    "        \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#reality\n",
    "# num_nodes=20\n",
    "# edge=[(0,18),(1,2),(2,3),(3,4),(4,5),(6,7),(7,8),(8,9),(10,11),(11,12),(12,13),(13,14),(11,15),(12,16),(16,17),\n",
    "#      (18,3),(3,7),(7,11),(11,15),(19,4),(4,8),(8,12)]\n",
    "\n",
    "\n",
    "num_nodes=370\n",
    "edge=np.load(\"edge.npy\")\n",
    "\n",
    "Graph=GraphConstruct(num_nodes,edge,max_hop=40,strategy='distance')\n",
    "A=Graph.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2., ..., 22., 23., 23.],\n",
       "       [ 1.,  0.,  1., ..., 21., 22., 22.],\n",
       "       [ 2.,  1.,  0., ..., 20., 21., 21.],\n",
       "       ...,\n",
       "       [22., 21., 20., ...,  0.,  1.,  1.],\n",
       "       [23., 22., 21., ...,  1.,  0.,  1.],\n",
       "       [23., 22., 21., ...,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Graph.hop_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 370, 370)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.网络构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, datasets, models\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CstGcn(tf.keras.Model):\n",
    "    def __init__(self,A,gcn_fliters=6,tcn_filters=4,gcc_filters=2):\n",
    "        super(CstGcn, self).__init__()\n",
    "        self.A=A\n",
    "        self.gcn_fliters=gcn_fliters\n",
    "        self.tcn_filters=tcn_filters\n",
    "        self.gcc_filters=gcc_filters\n",
    "        \n",
    "    \n",
    "    def conv2d(self,x,num_filter,kernel_size=(1,1),activation=tf.nn.relu,use_bn=False):\n",
    "        if use_bn:\n",
    "            x=layers.BatchNormalization(axis=1)(x)\n",
    "        x=layers.Conv2D(\n",
    "                        filters=num_filter,    \n",
    "                        kernel_size=kernel_size,         \n",
    "                        strides=(1, 1),\n",
    "                        padding='same',\n",
    "                        activation=tf.nn.relu,\n",
    "                        data_format='channels_last'\n",
    "                              )(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def gcn_res_block(self,x,conv_filter,A):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : (n,t,h,w,c)\n",
    "            A : (k,h,w)        \n",
    "        return\n",
    "            x : (n,t,h,w,f)\n",
    "        \"\"\"   \n",
    "        n,t,h,w,c=x.shape\n",
    "\n",
    "        k,h,w = A.shape\n",
    "\n",
    "        # forward feature when x, backwards when x=transpose(x)\n",
    "        x=self.conv2d(x,conv_filter*k)\n",
    "        x=tf.reshape(x,(-1,t,h,w,k,conv_filter))\n",
    "        x=tf.einsum('nthwkf, khw -> nthwf',x,A)\n",
    "\n",
    "        x_o = self.conv2d(x,1,(1,h))\n",
    "        x_d  = self.conv2d(x,1,(h,1))\n",
    "\n",
    "        x_o=self.conv2d(x_o, h,(1,1),activation=None)\n",
    "        x_o=tf.transpose(x_o,(0,1,2,4,3))\n",
    "\n",
    "        x_d=self.conv2d(x_d, h,(1,1),activation=None)\n",
    "        x_d=tf.transpose(x_d,(0,1,3,4,2))\n",
    "\n",
    "        x=tf.add(x_o,x_d)\n",
    "        x=self.conv2d(x,1)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def tcn_block(self,x,num_filter):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : (n,t,h,w,c)       \n",
    "        return\n",
    "            x : (n,h,w,1)\n",
    "        \"\"\"\n",
    "\n",
    "        x=layers.ConvLSTM2D(num_filter,kernel_size=(3,3), padding='same',data_format='channels_last',)(x)\n",
    "        x=self.conv2d(x,1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "    def gcc_block(self,x,num_filter):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x : (n,h,w,1)     \n",
    "        return\n",
    "            x : (n,h,w,1)\n",
    "        \"\"\"\n",
    "        res=x\n",
    "        x1=self.conv2d(x,num_filter)\n",
    "        x2=self.conv2d(x,num_filter)\n",
    "        x2=tf.transpose(x2,(0,2,1,3))\n",
    "#         n,f,h,w = x1.shape\n",
    "\n",
    "        x=tf.einsum('nhwf,nwqf -> nhqf',x1,x2)\n",
    "        x=tf.add(self.conv2d(x,1),res)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def call(self,x):\n",
    "        \n",
    "        #gcn\n",
    "        x=self.gcn_res_block(x,self.gcn_fliters,self.A)\n",
    "\n",
    "        x_T=tf.transpose(x,(0,1,3,2,4))\n",
    "        x_T=self.gcn_res_block(x_T,self.gcn_fliters,self.A)\n",
    "\n",
    "        x=tf.add(x,x_T)\n",
    "        x=tf.nn.relu(x)\n",
    "\n",
    "        #tcn\n",
    "        x=self.tcn_block(x,self.tcn_filters)\n",
    "\n",
    "        #gcc\n",
    "        x=self.gcc_block(x,self.gcc_filters)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "#eager模式消耗性能，但方便实时调试\n",
    "tf.config.run_functions_eagerly(False) \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep, nodes_num, adjacency_num, weather_dim = 5, 400, 40, 19\n",
    "\n",
    "# x = tf.constant(tf.constant_initializer()(shape=[3,5,370,370,1])) \n",
    "# A = tf.constant(tf.random_normal_initializer()(shape=[40,370,370]))\n",
    "\n",
    "x=np.random.rand(3,5,370,370,1)\n",
    "A=np.random.rand(40,370,370)\n",
    "\n",
    "y=np.random.rand(3,370,370,1)\n",
    "\n",
    "cstgcn = CstGcn(A)\n",
    "cstgcn.compile(optimizer='rmsprop', loss=tf.keras.losses.MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "2/2 [==============================] - 1s 590ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 2/10\n",
      "2/2 [==============================] - 1s 279ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 3/10\n",
      "2/2 [==============================] - 0s 248ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 4/10\n",
      "2/2 [==============================] - 0s 247ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 5/10\n",
      "2/2 [==============================] - 0s 247ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 6/10\n",
      "2/2 [==============================] - 0s 248ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 7/10\n",
      "2/2 [==============================] - 0s 248ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 8/10\n",
      "2/2 [==============================] - 0s 247ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 9/10\n",
      "2/2 [==============================] - 0s 248ms/sample - loss: 0.4957 - val_loss: 0.4951\n",
      "Epoch 10/10\n",
      "2/2 [==============================] - 0s 248ms/sample - loss: 0.4957 - val_loss: 0.4951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9868cf0b00>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "#设定格式化模型名称，以时间戳作为标记\n",
    "model_name = \"cstgcn\"\n",
    "#设定存储位置，每个模型不一样的路径\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(model_name))\n",
    "#使用它\n",
    "cstgcn.fit(x, y, batch_size =3, epochs=10, validation_split=0.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49549251794815063"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cstgcn.evaluate(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = cstgcn.predict(x[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43937906, 0.82145971, 0.18346909, ..., 0.53482477, 0.7519584 ,\n",
       "        0.21810749],\n",
       "       [0.3060722 , 0.15277989, 0.98317237, ..., 0.24283907, 0.97829012,\n",
       "        0.36325624],\n",
       "       [0.46616538, 0.65773883, 0.21093615, ..., 0.44083631, 0.54602933,\n",
       "        0.75992579],\n",
       "       ...,\n",
       "       [0.78135787, 0.52461333, 0.71817084, ..., 0.29784379, 0.2285722 ,\n",
       "        0.25069173],\n",
       "       [0.15512229, 0.27955503, 0.89679743, ..., 0.37042135, 0.10133943,\n",
       "        0.30115823],\n",
       "       [0.09663379, 0.14840225, 0.0379395 , ..., 0.09407211, 0.63912017,\n",
       "        0.60298632]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4945583811595566"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(y[0]-y_predict).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00550423, 0.        , 0.00303542, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.01350642, 0.        , 0.00223933, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.0032169 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.0216705 , 0.        , 0.00062624, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00140092],\n",
       "       [0.01608675, 0.        , 0.00087253, ..., 0.        , 0.        ,\n",
       "        0.0035792 ]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'time_distributed_1/Reshape_1:0' shape=(3, 5, 14, 14, 1) dtype=float32>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = layers.Conv2D(\n",
    "                    filters=1,    \n",
    "                    kernel_size=(1,1),         \n",
    "                    strides=(3, 3),\n",
    "                    activation=tf.nn.relu,\n",
    "                    data_format='channels_last'\n",
    "                          )\n",
    "\n",
    "layers.TimeDistributed(test)(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
