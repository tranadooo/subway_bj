{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "import os\n",
    "data_conn,cur = get_hive_con()\n",
    "tomorrow_info = pd.read_sql(\"select holiday_today,work,holiday_rest from bmnc_stados.bmnc_date_prop t where \\\n",
    "                                                    t.date_id='%s'\"%'20201001',data_conn)\n",
    "\n",
    "if tomorrow_info.loc[0,'holiday_rest'] !=1:\n",
    "    if tomorrow_info['work'].iloc[0]==1:\n",
    "        tomorrow='1'\n",
    "    elif tomorrow_info['work'].iloc[0]==0:\n",
    "        tomorrow='2'\n",
    "elif tomorrow_info.loc[0,'holiday_rest'] ==1:\n",
    "    tomorrow='3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "script_path = os.path.split(os.path.realpath('__file__'))[0]\n",
    "os.chdir(script_path)\n",
    "import sys\n",
    "\n",
    "from common import *\n",
    "\n",
    "import datetime as dt\n",
    "from  datetime import datetime\n",
    "def start_time_format(startTime):\n",
    "    start_time = datetime.strptime(compute_date+'%s:00'%startTime,'%Y%m%d%H:%M:%S')\n",
    "    start_time = start_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "    return start_time\n",
    "\n",
    "def end_time_format(endTime):\n",
    "    if endTime != '24:00':\n",
    "        end_time = datetime.strptime(compute_date+'%s:00'%endTime,'%Y%m%d%H:%M:%S')\n",
    "        end_time = (end_time+dt.timedelta(seconds=-1)).strftime(\"%Y%m%d%H%M%S\")\n",
    "    else:\n",
    "        end_time = compute_date+'235959'\n",
    "    return end_time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def get_data_by_sql(date_scope,exclude_date_scope,train_date_type,train_target,train_time_size):\n",
    "    sqlfile = '../sql/train1.sql'\n",
    "    \n",
    "    #节假日训练数据取双休日和节假日的历史数据合集\n",
    "    if train_date_type=='3':\n",
    "        sqlfile = '../sql/train3.sql'\n",
    "            \n",
    "    file = open(sqlfile,'r')\n",
    "    sql_content = file.read()\n",
    "\n",
    "    sql_train = sql_content.replace('datescope',date_scope).\\\n",
    "                                          replace('exclude',exclude_date_scope).\\\n",
    "                                          replace('work',train_date_type).\\\n",
    "                                          replace('target',train_target).\\\n",
    "                                          replace('timesize',train_time_size).\\\n",
    "                                          replace('\\n','').split(';')\n",
    "    print('train sql is: ')\n",
    "    for sql in sql_train:\n",
    "        print(sql)\n",
    "        cur.execute(sql)\n",
    "    data = cur.fetchall()\n",
    "    cols = [i[0] for i in cur.description]\n",
    "    pd_data = pd.DataFrame(data,columns=cols)\n",
    "    print(get_time(),'fetch over')\n",
    "\n",
    "    return pd_data\n",
    "\n",
    "\n",
    "def aggregation_data(pd_data, split_period):\n",
    "    for p in split_period:\n",
    "        s=p.split('-')[0]\n",
    "        e=p.split('-')[1]\n",
    "        pd_data.loc[(pd_data['start_tm']>=s) & (pd_data['end_tm']<=e),'newtime']= str([s,e])\n",
    "    pd_data['newtime'] = pd_data[['start_tm','end_tm','newtime']].apply(lambda x: str([x['start_tm'],x['end_tm']]) if pd.isna(x['newtime']) else x['newtime'], axis=1)\n",
    "    pd_data[['start_tm1','end_tm1']] = pd_data['newtime'].apply(lambda x: pd.Series(eval(x)))\n",
    "    pd_data = pd_data.rename(columns={'start_tm':'start_tm2','end_tm':'end_tm2','start_tm1':'start_tm','end_tm1':'end_tm'})\n",
    "    pd_data = pd_data.groupby(['line_id','station_id','start_tm','end_tm'])['qtty'].apply(','.join).reset_index()\n",
    "    pd_data['qtty']=pd_data['qtty'].apply(lambda x : '[%s]'%x)\n",
    "    return pd_data\n",
    "\n",
    "\n",
    "\n",
    "def iforest_func(subset,contamination,safe_value):\n",
    "    subset[\"detect_tmp\"] = subset[\"qtty\"].apply(lambda x:main(x,contamination,safe_value))\n",
    "    return subset\n",
    "\n",
    "def max_func(subset,safe_value):\n",
    "    subset[\"detect_tmp\"] = subset[\"qtty\"].apply(lambda x:(max(eval(x)),max([safe_value,max(eval(x))]),np.array([])))\n",
    "    return subset\n",
    "\n",
    "import sys\n",
    "def train(pd_data,algorithm_mark,alg_para_list,size,split_period):\n",
    "\n",
    "    #计算阈值\n",
    "    print(get_time(),'compute')\n",
    "    pd_data_grouped = pd_data.groupby(pd_data.index)\n",
    "    if len(alg_para_list)!=0:\n",
    "        results = Parallel(n_jobs=30)(delayed(eval('%s_func'%algorithm_mark))(group,*alg_para_list) for name, group in pd_data_grouped)\n",
    "    else:\n",
    "        results = Parallel(n_jobs=30)(delayed(eval('%s_func'%algorithm_mark))(group) for name, group in pd_data_grouped)\n",
    "    pd_data = pd.concat(results)  \n",
    "    print(get_time(),'computed')\n",
    "\n",
    "    #统计训练集异常明细\n",
    "    print(get_time(),\"statistic outliners detail\")\n",
    "    pd_data[['threshold','modified','outliners']]=pd_data['detect_tmp'].apply(pd.Series)\n",
    "    pd_data['train_n'] = pd_data['qtty'].apply(lambda x:len(eval(x)))\n",
    "    pd_data['outliner_n'] = pd_data['outliners'].apply(lambda x:len(x))\n",
    "    pd_data['outliner_p'] = pd_data[['outliner_n','train_n']].apply(lambda x:'%0.5f'%(x['outliner_n']/x['train_n']),axis=1)    \n",
    "    \n",
    "    #检查是否有聚合，如果有聚合就还原聚合部分\n",
    "    if split_period!=[]:\n",
    "        print(get_time(),\"revivifiction time period\")   \n",
    "        resplit_pd = get_splitmap_by_timesize(split_period,size=int(size))\n",
    "        resplit_pd.columns=['nst','net','start_tm','end_tm']\n",
    "        pd_data = pd.merge(pd_data,resplit_pd,on=['start_tm','end_tm'],how='left')\n",
    "        pd_data = pd_data.rename(columns={'start_tm':'ost','end_tm':'oet','nst':'start_tm','net':'end_tm'})\n",
    "        pd_data['start_tm'] = pd_data[['ost','start_tm']].apply(lambda x:x['ost'] if pd.isna(x['start_tm']) else x['start_tm'] ,axis=1)\n",
    "        pd_data['end_tm'] = pd_data[['oet','end_tm']].apply(lambda x:x['oet'] if pd.isna(x['end_tm']) else x['end_tm'] ,axis=1)\n",
    "\n",
    "\n",
    "    #补全时段\n",
    "    print(get_time(),'add miss period')\n",
    "    periods = get_period_by_timesize(int(size))\n",
    "    starts = list(periods.keys())\n",
    "    line_station_arr  = pd_data[['line_id','station_id']].drop_duplicates().values\n",
    "    for line,station in line_station_arr:\n",
    "        cur_start = pd_data.loc[pd_data['station_id']==station,'start_tm'].values\n",
    "        miss_starts = list(set(starts)-set(cur_start))\n",
    "        if len(miss_starts)!=0:\n",
    "            for miss_start in miss_starts:\n",
    "                miss_end = periods[miss_start]\n",
    "                miss_list = [line,station,miss_start,miss_end,'[]',99999,99999,0,0,0.00000,'']\n",
    "                rows = pd_data.shape[0]\n",
    "                pd_data.loc[rows,['line_id','station_id','start_tm','end_tm','qtty','threshold','modified','train_n','outliner_n','outliner_p','outliners']]=miss_list\n",
    "\n",
    "    pd_data['threshold'] = pd_data['threshold'].astype(int)\n",
    "    pd_data['modified'] = pd_data['modified'].astype(int)\n",
    "  \n",
    "    print(get_time(),'complete handle %s'%size)\n",
    "    \n",
    "    return pd_data\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--train_date_scopes\",type=str) #训练日期范围\n",
    "parser.add_argument(\"--exclude_date_scopes\",type=str) #过滤日期范围\n",
    "\n",
    "parser.add_argument(\"--train_date_types\",type=str) #训练日期类型\n",
    "parser.add_argument(\"--train_targets\",type=str) #训练指标\n",
    "parser.add_argument(\"--train_time_sizes\",type=str) #训练指标粒度\n",
    "\n",
    "\n",
    "import hashlib\n",
    "import traceback\n",
    "import time\n",
    "        \n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    data_conn,cur = get_hive_con()\n",
    "    \n",
    "    print('\\n',get_time(),'Start Job \\nset defualt parameters with config file')\n",
    "    f=open('../conf/train_conf.py','r')\n",
    "    for i in f:\n",
    "        print(i)\n",
    "    sys.path.append('../conf')\n",
    "    from train_conf import *\n",
    "\n",
    "\n",
    "#     args = parser.parse_args()\n",
    "#     #如果有参数是命令行指定的，那么用它覆盖现有参数\n",
    "#     options = vars(args)\n",
    "#     for arg in options:\n",
    "#         value=options[arg]\n",
    "#         if value is not None:\n",
    "#             if arg==\"train_date_scopes\":\n",
    "#                 locals()[arg]=str(value).split(',')\n",
    "#             elif arg==\"exclude_date_scopes\":\n",
    "#                 locals()[arg]=str(value).split('#')\n",
    "#             else:\n",
    "#                 locals()[arg]=eval(value)\n",
    "#             print('specially set parameter with cmd',arg,locals()[arg])\n",
    "\n",
    "         \n",
    "    tomorrow_type=get_tomorrow_daytype()\n",
    "    if train_frequency=='1':\n",
    "#         train_date_types=[tomorrow_type]\n",
    "        train_date_types=['3']\n",
    "    elif train_frequency=='7':\n",
    "        train_date_types=['1','2','3']  #训练日期类型：1工作日，2双休日，3节假日\n",
    "        \n",
    "    import itertools\n",
    "    combinations = list(itertools.product(train_date_scopes,exclude_date_scopes,train_date_types,\n",
    "                                              train_targets,train_time_sizes))\n",
    "\n",
    "    task_n = len(combinations);count=1\n",
    "    \n",
    "    \n",
    "    for date_scope,exclude_date_scope,train_date_type,train_target,train_time_size in combinations:\n",
    "        \n",
    "        \n",
    "        train_stratage = {**train_stratage_config['common'],**train_stratage_config[(train_date_type,train_target,train_time_size)]}       \n",
    "        sess = get_spark_sess()\n",
    "        \n",
    "        print('\\n',get_time(),'\\n#####start training %s/%s#######'%(count,task_n))\n",
    "        print( 'date_scope--%s\\nexclude_date_scope--%s\\\n",
    "                 \\ntrain_date_type--%s\\ntrain_target--%s\\\n",
    "                 \\ntrain_time_size--%s\\n'% \n",
    "                 (date_scope,exclude_date_scope,train_date_type,train_target,train_time_size))\n",
    "        \n",
    "        \n",
    "        pd_data_ori = get_data_by_sql(date_scope,exclude_date_scope,train_date_type,train_target,train_time_size)\n",
    "        pd_data=pd.DataFrame([])\n",
    "        for k,v in train_stratage.items():\n",
    "            split_period = v[0]\n",
    "            if k=='main':\n",
    "                rest_key = [k for k,v in train_stratage.items() if k!='main']\n",
    "                rest_key = [[i] if type(i)==str else list(i) for i in rest_key]\n",
    "                if rest_key == []:\n",
    "                    cur_data = pd_data_ori\n",
    "                elif len(rest_key)==1:    \n",
    "                    cur_data = pd_data_ori.loc[~pd_data_ori['station_id'].isin(rest_key[0])].reset_index(drop=True)\n",
    "                elif len(rest_key)>1:\n",
    "                    cur_data = pd_data_ori.loc[~pd_data_ori['station_id'].isin(np.sum(rest_key))].reset_index(drop=True)\n",
    "            else:\n",
    "                if type(k)==str:\n",
    "                    k=[k]\n",
    "                cur_data = pd_data_ori.loc[pd_data_ori['station_id'].isin(k)].reset_index(drop=True)\n",
    "\n",
    "            if split_period==[]:\n",
    "                train_plan_id = '1'\n",
    "                cur_data['qtty']=cur_data['qtty'].apply(lambda x : '[%s]'%x)\n",
    "            elif split_period!=[]:\n",
    "                train_plan_id = '2'\n",
    "                cur_data = aggregation_data(cur_data, split_period)            \n",
    "            else:\n",
    "                print(\"no stratage configed\")\n",
    "\n",
    "            print('\\n',get_time(),k,len(cur_data),v[0],v[1],train_time_size)\n",
    "            cur_data = train(cur_data,v[1][0],v[1][1:],train_time_size,split_period)\n",
    "            pd_data = pd.concat([pd_data,cur_data])\n",
    "\n",
    "        try:         \n",
    "            main_stratage = train_stratage['main']\n",
    "        except Exception as e:\n",
    "            main_stratage = v     \n",
    "        alg_with_para=\",\".join([str(i) for i in main_stratage[1]])\n",
    "        \n",
    "        if main_stratage[0]==[]:\n",
    "            train_plan_id = '1'\n",
    "        else:\n",
    "            train_plan_id = '2'\n",
    "        \n",
    "        id_str = str([date_scope,exclude_date_scope,train_plan_id,train_date_type,\\\n",
    "               train_target,train_time_size,alg_with_para])\n",
    "        md5str = hashlib.md5(id_str.encode(encoding='UTF-8')).hexdigest()\n",
    "        task_id = md5str\n",
    "        #插入训练任务信息表\n",
    "        train_info_sql = \"insert overwrite table bmnc_stados.train_his_record partition(train_id='%s') \\\n",
    "                                                values('%s','%s','%s','%s','%s','%s','%s','%s') \\\n",
    "                                    \"%(task_id,get_time(),date_scope,exclude_date_scope,train_plan_id,train_date_type,\\\n",
    "                                       train_target,train_time_size,alg_with_para)\n",
    "        print(train_info_sql)\n",
    "        cur.execute(train_info_sql)\n",
    "        print(get_time(),'insert train record finished')\n",
    "        #pd_data['date_type'] = train_date_type\n",
    "        #pd_data['train_target'] = train_target\n",
    "        #pd_data['train_time_size'] = train_time_size\n",
    "\n",
    "        pd_data = pd_data[['line_id','station_id','start_tm','end_tm','threshold','modified','train_n','outliner_n','outliner_p','outliners']]\n",
    "        pd_data['outliners']=pd_data['outliners'].apply(lambda x:str(list(x)).replace('[','').replace(']',''))\n",
    "        pd_data['threshold'] = pd_data['threshold'].astype(str)\n",
    "        pd_data['modified'] = pd_data['modified'].astype(str)\n",
    "        pd_data['train_n'] = pd_data['train_n'].astype(int).astype(str)\n",
    "        pd_data['outliner_n'] = pd_data['outliner_n'].astype(int).astype(str)\n",
    "        pd_data['outliner_p']=pd_data['outliner_p'].astype(str)\n",
    "        \n",
    "        \n",
    "        spark_df = sess.createDataFrame(pd_data.values.tolist(), list(pd_data.columns))    \n",
    "        hdfs_file='hdfs:/user/hive/warehouse/bmnc_stados.db/train_result_detail/train_id=%s'%task_id\n",
    "        print(get_time(),'Insert write to hdfs %s'%hdfs_file)\n",
    "        \n",
    "        for j in range(1,4):\n",
    "            try:    \n",
    "                spark_df.write.mode(\"overwrite\").option(\"delimiter\", \"|\").format('csv').save(hdfs_file)\n",
    "                print(\"success in %s times\"%j)\n",
    "                break\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"#######\")\n",
    "                print(traceback.format_exc())\n",
    "                time.sleep(3)\n",
    "        \n",
    "        print(get_time(),'hdfs file writed')\n",
    "        cur.execute('msck repair table bmnc_stados.train_result_detail')\n",
    "        \n",
    "        count=count+1\n",
    "        \n",
    "    print(\"All task complete\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
